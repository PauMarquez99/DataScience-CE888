{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albanda/CE888/blob/master/lab5/bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G_ohAqyeAWh5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NyfMa-ZzAWia"
      },
      "outputs": [],
      "source": [
        "# Define our actions\n",
        "\n",
        "def action_0():\n",
        "    return np.random.choice([1, 0], p=[0.5, 0.5])\n",
        "\n",
        "def action_1():\n",
        "    return np.random.choice([1, 0], p=[0.6, 0.4])\n",
        "\n",
        "def action_2():\n",
        "    return np.random.choice([1, 0], p=[0.2, 0.8])\n",
        "\n",
        "rewards = [action_0, action_1, action_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OMYEfKiAWig",
        "outputId": "ad9c214b-9280-4dfc-8438-43a5cd1e873c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pull 0 (action_0): reward=0\n",
            "Pull 1 (action_0): reward=0\n",
            "Pull 2 (action_0): reward=1\n",
            "Pull 3 (action_0): reward=0\n",
            "Pull 4 (action_0): reward=0\n",
            "Pull 5 (action_0): reward=0\n",
            "Pull 6 (action_0): reward=0\n",
            "Pull 7 (action_0): reward=0\n",
            "Pull 8 (action_0): reward=0\n",
            "Pull 9 (action_0): reward=0\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print('Pull %d (action_0): reward=%d' % (i, rewards[0]()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVjefuf9AWip",
        "outputId": "019cdf1a-5f03-42a9-ea28-24309642c467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action 0: Q(a_0)=0.50\n",
            "Action 1: Q(a_1)=0.60\n",
            "Action 2: Q(a_2)=0.20\n"
          ]
        }
      ],
      "source": [
        "# Simulate action values (Q): expected reward for each action\n",
        "pulls = 100000\n",
        "\n",
        "action_values = []\n",
        "for reward in rewards:\n",
        "    value = [reward() for _ in range(pulls)]  # execute each of the actions 'pulls' times\n",
        "    action_values.append(value)\n",
        "\n",
        "for action, value in enumerate(action_values):\n",
        "    print(\"Action %d: Q(a_%d)=%.2f\" % (action, action, np.mean(value)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6wY2EchRAWiu"
      },
      "outputs": [],
      "source": [
        "# To simulate the values (V), we need to define a policy\n",
        "# (Value is the expected reward given the policy I'm following)\n",
        "\n",
        "# Define a policy:\n",
        "def policy_random():\n",
        "    '''Returns which action to perform using equal probabilities for each action'''\n",
        "    return np.random.choice([0, 1, 2], p=[1/3, 1/3, 1/3])\n",
        "\n",
        "\n",
        "def policy_better():\n",
        "    ''' A better policy than random: we choose actions 0 and 1 more often than action 2'''\n",
        "    return np.random.choice([0, 1, 2], p=[0.4, 0.5, 0.1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn70-2BnAWiy",
        "outputId": "bc241f46-3ead-4ebb-b05f-baacc3bbd935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward = 42906\n",
            "Average reward: V = 0.42906\n"
          ]
        }
      ],
      "source": [
        "# Simulate Values using the random policy\n",
        "total_reward = 0\n",
        "for pull in range(pulls):\n",
        "    action = policy_random()\n",
        "    total_reward += rewards[action]()\n",
        "print(\"Total reward =\", total_reward)\n",
        "print(\"Average reward: V =\", total_reward/pulls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-6gLUeWAWi3",
        "outputId": "63bd85df-d7d6-4862-db62-6c8a83264039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward = 52134\n",
            "Average reward: V = 0.52134\n"
          ]
        }
      ],
      "source": [
        "# Simulate Values using the better policy\n",
        "total_reward = 0\n",
        "for pull in range(pulls):\n",
        "    action = policy_better()\n",
        "    total_reward += rewards[action]()\n",
        "print(\"Total reward =\", total_reward)\n",
        "print(\"Average reward: V =\", total_reward/pulls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vML7aYcDAWi6",
        "outputId": "b58e55a3-ae19-4352-dfc2-41533d514317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V* = 0.59985\n",
            "Regret: I_t = 0.08\n"
          ]
        }
      ],
      "source": [
        "# Regret of the better policy\n",
        "V_star = max([np.mean(value) for value in action_values])\n",
        "print(\"V* =\", V_star)\n",
        "\n",
        "total_regret = 0\n",
        "for pull in range(pulls):\n",
        "    total_regret += (V_star - rewards[policy_better()]())\n",
        "print('Regret: I_t = %.2f' % (total_regret/pulls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pkhEOOz8AWi_"
      },
      "outputs": [],
      "source": [
        "# Some bandit policies to explore:\n",
        "\n",
        "def policy_greedy(action_values):\n",
        "    '''Always returns the action for which the payoff is highest'''\n",
        "    best_action = np.argmax([np.mean(value) for value in action_values])\n",
        "    return best_action\n",
        "\n",
        "\n",
        "def policy_e_greedy(action_values, epsilon=0.05):\n",
        "    '''We explore with epsilon probability, and choose the best action the rest of the time'''\n",
        "    explore = np.random.choice([1, 0], p=[epsilon, 1-epsilon])\n",
        "    if explore:\n",
        "        # Random action\n",
        "        return policy_random()\n",
        "    else:\n",
        "        # Choose best action\n",
        "        return policy_greedy(action_values)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wyGqo1sTAWjC"
      },
      "outputs": [],
      "source": [
        "# Implementing the decaying epsilon-greedy properly requires a class definition so we can store the epsilon values\n",
        "class DecayingEGreedy:\n",
        "    \n",
        "    def __init__(self, epsilon, decay=0.99, lower_bound=0):\n",
        "        self.epsilon = epsilon\n",
        "        self.decay = decay\n",
        "        self.lower_bound = lower_bound\n",
        "        \n",
        "    def policy(self, action_values):\n",
        "        if self.lower_bound > 0 and self.epsilon > self.lower_bound:\n",
        "            self.epsilon *= self.decay  # update epsilon\n",
        "        explore = np.random.choice([1, 0], p=[self.epsilon, 1-self.epsilon])  # explore vs exploit decision\n",
        "        if explore:\n",
        "            # Random action\n",
        "            return policy_random()\n",
        "        else:\n",
        "            # Choose best action\n",
        "            return policy_greedy(action_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fT5ZzbfZAWjE",
        "outputId": "1178bb3e-70e0-4126-dfc1-f9f24c6f3568",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of pulls\t\tTotal reward\t\tV\n",
            "1000\t\t\t597\t\t\t0.598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pauil\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "C:\\Users\\pauil\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\t\t\t1211\t\t\t0.606\n",
            "3000\t\t\t1775\t\t\t0.592\n",
            "4000\t\t\t2378\t\t\t0.595\n",
            "5000\t\t\t2966\t\t\t0.593\n",
            "6000\t\t\t3582\t\t\t0.597\n",
            "7000\t\t\t4150\t\t\t0.593\n",
            "8000\t\t\t4738\t\t\t0.592\n",
            "9000\t\t\t5359\t\t\t0.596\n",
            "10000\t\t\t5961\t\t\t0.596\n",
            "11000\t\t\t6560\t\t\t0.596\n",
            "12000\t\t\t7139\t\t\t0.595\n",
            "13000\t\t\t7754\t\t\t0.597\n",
            "14000\t\t\t8374\t\t\t0.598\n",
            "15000\t\t\t8990\t\t\t0.599\n",
            "16000\t\t\t9598\t\t\t0.600\n",
            "17000\t\t\t10214\t\t\t0.601\n",
            "18000\t\t\t10784\t\t\t0.599\n",
            "19000\t\t\t11390\t\t\t0.600\n",
            "20000\t\t\t11993\t\t\t0.600\n",
            "21000\t\t\t12622\t\t\t0.601\n",
            "22000\t\t\t13222\t\t\t0.601\n",
            "23000\t\t\t13805\t\t\t0.600\n",
            "24000\t\t\t14352\t\t\t0.598\n",
            "25000\t\t\t14935\t\t\t0.597\n",
            "26000\t\t\t15540\t\t\t0.598\n",
            "27000\t\t\t16113\t\t\t0.597\n",
            "28000\t\t\t16709\t\t\t0.597\n",
            "29000\t\t\t17301\t\t\t0.597\n",
            "30000\t\t\t17921\t\t\t0.597\n",
            "31000\t\t\t18510\t\t\t0.597\n",
            "32000\t\t\t19097\t\t\t0.597\n",
            "33000\t\t\t19681\t\t\t0.596\n",
            "34000\t\t\t20257\t\t\t0.596\n",
            "35000\t\t\t20857\t\t\t0.596\n",
            "36000\t\t\t21455\t\t\t0.596\n",
            "37000\t\t\t22033\t\t\t0.596\n",
            "38000\t\t\t22607\t\t\t0.595\n",
            "39000\t\t\t23173\t\t\t0.594\n",
            "40000\t\t\t23762\t\t\t0.594\n",
            "41000\t\t\t24340\t\t\t0.594\n",
            "42000\t\t\t24922\t\t\t0.593\n",
            "43000\t\t\t25530\t\t\t0.594\n",
            "44000\t\t\t26148\t\t\t0.594\n",
            "45000\t\t\t26737\t\t\t0.594\n",
            "46000\t\t\t27342\t\t\t0.594\n",
            "47000\t\t\t27944\t\t\t0.595\n",
            "48000\t\t\t28524\t\t\t0.594\n",
            "49000\t\t\t29113\t\t\t0.594\n",
            "50000\t\t\t29717\t\t\t0.594\n",
            "51000\t\t\t30308\t\t\t0.594\n",
            "52000\t\t\t30890\t\t\t0.594\n",
            "53000\t\t\t31483\t\t\t0.594\n",
            "54000\t\t\t32077\t\t\t0.594\n",
            "55000\t\t\t32675\t\t\t0.594\n",
            "56000\t\t\t33290\t\t\t0.594\n",
            "57000\t\t\t33902\t\t\t0.595\n",
            "58000\t\t\t34498\t\t\t0.595\n",
            "59000\t\t\t35077\t\t\t0.595\n",
            "60000\t\t\t35665\t\t\t0.594\n",
            "61000\t\t\t36248\t\t\t0.594\n",
            "62000\t\t\t36841\t\t\t0.594\n",
            "63000\t\t\t37430\t\t\t0.594\n",
            "64000\t\t\t38031\t\t\t0.594\n",
            "65000\t\t\t38645\t\t\t0.595\n",
            "66000\t\t\t39254\t\t\t0.595\n",
            "67000\t\t\t39839\t\t\t0.595\n"
          ]
        }
      ],
      "source": [
        "# Let's test the decaying epsilon-greedy approach\n",
        "agent = DecayingEGreedy(epsilon=0.1, decay=0.99, lower_bound=0.03)\n",
        "\n",
        "# Full problem:\n",
        "action_values = [[], [], []] # initialise values\n",
        "rewards_decaying_e_greedy = []\n",
        "total_reward = 0\n",
        "print('Number of pulls\\t\\tTotal reward\\t\\tV')\n",
        "for pull in range(pulls):\n",
        "    action = agent.policy(action_values)  # choose action according to policy\n",
        "    reward = rewards[action]()  # get reward\n",
        "    action_values[action].append(reward)  # update action_values so we make better decisions down the line\n",
        "    total_reward += reward\n",
        "    if (pull+1) % 1000 == 0:\n",
        "        print('%d\\t\\t\\t%d\\t\\t\\t%.3f' % (pull+1, total_reward, total_reward/pull))\n",
        "        rewards_decaying_e_greedy.append(total_reward/pull)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLHnVY5AAWjL",
        "outputId": "f2413923-45c5-4d38-f3a1-917922223850"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(1000, pulls+1, step=1000), rewards_decaying_e_greedy)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average reward (V)\")\n",
        "# The average reward is 0.594, which is very close to V* (0.6)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeR_XuqUAWjO"
      },
      "outputs": [],
      "source": [
        "# Implementing the decaying epsilon-greedy properly requires a class definition so we can store the epsilon values\n",
        "class UCB:\n",
        "    \n",
        "    def __init__(self, C=0.5, n_arms=3):\n",
        "        self.C = C\n",
        "        self.pulls = 0\n",
        "        self.counts = np.asarray([0] * n_arms)\n",
        "        \n",
        "    def update_counts(self, arm):\n",
        "        self.pulls += 1\n",
        "        self.counts[arm] += 1\n",
        "        \n",
        "    def policy(self, action_values):\n",
        "        action_values = np.asarray([np.mean(value) for value in action_values])\n",
        "        uncertainty = np.sqrt(np.log(self.pulls) / self.counts)\n",
        "        ucb = action_values + self.C * uncertainty\n",
        "        action = np.argmax(ucb)\n",
        "        self.update_counts(action)\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRI6_3tCAWjP",
        "outputId": "319db213-4351-4641-c6e3-d0fcb47d6e5f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Let's test the decaying epsilon-greedy approach\n",
        "agent = UCB(C=0.5)\n",
        "\n",
        "# Full problem:\n",
        "action_values = [[], [], []] # initialise values\n",
        "total_reward = 0 # reset reward\n",
        "rewards_ucb = []\n",
        "print('Number of pulls\\t\\tTotal reward\\t\\tV')\n",
        "for pull in range(pulls):\n",
        "    action = agent.policy(action_values)  # choose action according to policy\n",
        "    reward = rewards[action]()  # get reward\n",
        "    action_values[action].append(reward)  # update action_values so we make better decisions down the line\n",
        "    total_reward += reward\n",
        "    if (pull+1) % 1000 == 0:\n",
        "        print('%d\\t\\t\\t%d\\t\\t\\t%.3f' % (pull+1, total_reward, total_reward/pull))\n",
        "        rewards_ucb.append(total_reward/pull)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT8pNLSlAWjR",
        "outputId": "391c7d26-5f2f-4319-e22c-533fd42d4e3f"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(1000, pulls+1, step=1000), rewards_ucb)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average reward (V)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgfnlMU6AWjT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "bandits.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
